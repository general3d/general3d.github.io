<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="General3D: General 3D Talking Face Generation with In-Context Talking Style Control">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>General3D: General 3D Talking Face Generation with In-Context Talking Style Control</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
<style> .table-warped {overflow:scroll;}</style>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <h2 class="title is-1 publication-title">
          General3D: General 3D Talking Face Generation with In-Context Talking Style Control</h2>
        <div class="is-size-5 publication-authors">
          <span class="author-block">
            Anonymous Authors</span>
        </div>
        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Code Link. -->
            <span class="link-block">
              <a href="" class="external-link button is-normal is-rounded is-dark" onclick="alert('We plan to release the source code after the rebuttal phase.')">
              <span class="icon">
              <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
              </span>
              <span>Code</span>
              </a>
              </span>
          </div>
        </div>
      </div>  
    </div>
    

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Talking face generation (TFG) aims to animate a target identity's face to create realistic talking videos using an image, multiple images, or a video clip as the identity source. However, finding a generic and scalable way to represent the human head for the different identity sources has been challenging. As a result, the TFG community has independently developed specialized methodologies for each setting. We argue that an ideal TFG system should handle all types of identity sources and harness each methodology's strengths. In this study, we propose General3D, the first attempt to unify TFG into a general framework. Our approach consists of three main components: (1) a coarse-to-fine generic (CTFG) talking face model that supports any identity source without requiring adaptation, (2) a static-dynamic-hybrid (SD-Hybrid) pipeline that enables fast and sample efficient adaptation of our CTFG model to achieve higher identity similarity on unseen identities (47 times faster than baselines), and (3) an in-context stylized audio-to-motion (ICS-A2M) model that enables generating co-speech facial motion with a personalized talking style. Extensive experiments demonstrate that General3D surpasses existing baselines and produces realistic talking-face videos across all TFG settings.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Overall Framework -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overall Framework</h2>
        <div class="content has-text-justified">
          <p>
            The overall inference process of General3D is demonstrated as follows:
          </p>
          <p>
            <img src="./static/images/inference.png"
      class="interpolation-image"
      alt="The inference process of General3D."/>
          </p>
        </div>
      </div>
    </div>
    <!--/ Overall Framework -->
  </div>
<!-- </section> -->
  <div> </div>



<!-- <section class="section"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered ">
      <div class="column is-four-fifths">
        <h2 class="title is-3 ">1. Quick Intro & Qualitative Demos in 5 minutes</h2>
        <p>
          In this video, we first give a brief <u><b>system overview</b></u>, then we provide three demos to demonstrate the effectiveness of the (1) coarse-to-fine generic <u><b>(CTFG)</b></u> talking face model, (2) static-dynamic hybrid <u><b>(SD-Hybrid)</b></u> adaptation pipeline, and (3) in-context stylized audio-to-motion <b>(ICS-A2M)</b> model. 
        </p>
        <video id="main-demo-video"
                controls
                preload
                playsinline
                width="100%">
          <source src="./static/videos/general3d_main_demo.mp4"
                  type="video/mp4">
        </video>
      </div>
    </div>


    <div class="columns ">
      <div class="column ">
        <div class="content ">
          <h2 class="title is-3 has-text-centered ">2. Comparison with Baselines</h2>
          <h3 class="title is-4 has-text-centered ">2.1 One/Few-shot Video-Driven Methods</h3>
          <div><b>Tested Baselines: </b> </div>
          <div>
            <b>One-shot: Face-vid2vid (CVPR 2021), TPS (CVPR 2022), DPE (CVPR 2023), HiDe-NeRF (CVPR 2023);</b>
          </div>
          <b>Few-shot: FewShot-vid2vid (NeurIPS 2019)</b>
          <p></p>
          From the following demo video, we can see that our General3D has the following advantages over previous video-driven baselines: (1) It can maintain realistic 3D geometry under larger motion, whereas previous baselines like Face-vid2vid may produce distortion or warping artifacts; (2) It can generate more realistic regions that are occluded in the source image, such as eyes, teeth, and side faces; (3) It has better overall image quality.
          <p></p>

        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <video controls preload width="70%">
        <source src="static/videos/comparison_with_video_driven.mp4" type="video/mp4" > 
      </video>
    </div>
    <div> <br> </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-justified is-centered">
          <!-- <div> <p> </p> </div> -->
          <h3 class="title is-5 has-text-centered">2.2 One-Shot Audio-Driven Methods</h3>

          <div><b>MakeItTalk (SIGGRAPH Asia 2020), PC-AVS (CVPR 2021),</b></div>
          <div><b>SadTalker (CVPR 2023) </b></div>
          <p></p>
          Our method achieves more accurate lip-sync, better visual quality, and better identity similarity at various head poses.
        </div>
        <div class="is-centered">
          <video controls preload width="80%" center>
            <source src="static/videos/comparison_with_oneshot_audio_driven.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>

      <div class="column is-centered">
        <div class="content has-text-justified is-centered">
          <!-- <div> <p> </p> </div> -->
          <h3 class="title is-5 has-text-centered ">2.3 Person-Specific Audio-Driven Methods</h3>
          <div><b>GeneFace (ICLR 2023), RAD-NeRF (Arxiv 2022.12),</b></div>
          <div><b>ER-NeRF (ICCV 2023)</b></div>
          <p></p>
          Our method produces more temporal stable and lip-synced results than NeRF-based baselines, while achieving comparable visual quality.
        </div>
        <div class="is-centered">

          <video controls preload width="100%">
            
            <source src="static/videos/comparison_with_nerf_audio_driven.mp4" width="60%" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <h2 class="title is-3 is-centered">3. More Demos</h2>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-5">3.1 More Few-shot talking face generation with the <b><u>CTFG model</u></b></h3>
          <video controls preload width="100%">
            <source src="static/videos/demo_ctfg.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>

      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-5">3.2 More person-specific talking face generation with the <b><u>SD-Hybrid adaptation</u></b></h3>
          <video controls preload width="80%">
            
            <source src="static/videos/demo_sd_hybrid.mp4" width="60%" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">3.3 Stylized and Expressive Co-Speech Motion Generation with the ICS-A2M model</h3>
          <video controls preload width="75%">
            <source src="static/videos/demo_ics_a2m.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-centered">
        <div class="content has-text-centered is-centered">
          <h3 class="title is-4">3.4 Interpretability: How the CTFG model works in the few-shot setting</h3>
          <video controls preload width="75%">
            <source src="static/videos/demo_ctfg_attention.mp4" type="video/mp4"> 
          </video>
        </div>
      </div>
    </div>




  </div>
</section>



</body>
</html>
